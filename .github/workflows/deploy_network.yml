name: Deploy Network

on:
  workflow_dispatch:

jobs:
  deploy-network:
    runs-on: ubuntu-latest
    steps:
      # Étape 1 : Checkout du dépôt
      - name: Checkout repository
        uses: actions/checkout@v2

      # Étape 2 : Installer Helm (si nécessaire)
      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      # Étape 3 : Installer Scaleway CLI
      - name: Install Scaleway CLI
        run: |
          curl -s https://raw.githubusercontent.com/scaleway/scaleway-cli/master/scripts/get.sh | sh
          scw version

      # Étape 4 : Configurer Scaleway CLI
      - name: Configure Scaleway CLI
        env:
          SCW_ACCESS_KEY: ${{ secrets.SCW_ACCESS_KEY }}
          SCW_SECRET_KEY: ${{ secrets.SCW_SECRET_KEY }}
          SCW_DEFAULT_PROJECT_ID: ${{ secrets.SCW_PROJECT_ID }}
          SCW_DEFAULT_ORGANIZATION_ID: ${{ secrets.SCW_ORGANIZATION_ID }}
          SCW_DEFAULT_REGION: fr-par
          SCW_DEFAULT_ZONE: fr-par-1
        run: |
          mkdir -p ~/.config/scw
          touch ~/.config/scw/config.yaml
          scw config set access-key="${SCW_ACCESS_KEY}"
          scw config set secret-key="${SCW_SECRET_KEY}"
          scw config set default-organization-id="${SCW_DEFAULT_ORGANIZATION_ID}"
          scw config set default-project-id="${SCW_DEFAULT_PROJECT_ID}"
          scw config set default-region="${SCW_DEFAULT_REGION}"
          scw config set default-zone="${SCW_DEFAULT_ZONE}"

          # Vérification de la configuration
          scw config get default-region
          scw config get default-zone
          scw config get access-key

          # Exporter explicitement la région et la zone pour être certain qu'elles sont prises en compte
          echo "SCW_DEFAULT_REGION=${SCW_DEFAULT_REGION}" >> $GITHUB_ENV
          echo "SCW_DEFAULT_ZONE=${SCW_DEFAULT_ZONE}" >> $GITHUB_ENV

      - name: Create Flow Logs Bucket
        run: |
          BUCKET_NAME="flow-logs-$(date +%s)"
          echo "Creating bucket: $BUCKET_NAME"
          
          # Vérifier si la région est bien définie
          if [ -z "${SCW_DEFAULT_REGION}" ]; then
            echo "Error: SCW_DEFAULT_REGION is not set or is empty"
            exit 1
          fi
          
          # Créer un bucket Object Storage avec la bonne région
          scw object bucket create "$BUCKET_NAME" region="$SCW_DEFAULT_REGION" enable-versioning=false acl=private
          
          # Stocker le nom du bucket dans une variable d'environnement pour les étapes suivantes
          echo "BUCKET_NAME=$BUCKET_NAME" >> $GITHUB_ENV



      # Étape 5 : Configurer kubeconfig
      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          cluster_id=$(scw k8s cluster list name=k8s-cluster project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[0].id')
          if [[ -z "$cluster_id" || "$cluster_id" == "null" ]]; then
            echo "Error: Kubernetes cluster ID not found or is not valid."
            exit 1
          fi
          echo "Cluster ID: $cluster_id"
          scw k8s kubeconfig get "$cluster_id" > ~/.kube/config

      # Étape 7 : Clean des Ressources
      - name: Delete network resources
        continue-on-error: true
        run: |
          kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml -n ingress-nginx
          kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml -n cert-manager
          kubectl delete -f ./Ingress/clusterIssuer.yaml -n default
          kubectl delete -f ./Ingress/network.yaml
          kubectl delete ns ingress-nginx --force --grace-period=0



          

      - name: Deploy NGINX Ingress Controller
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            -f ./helm/values-nginx.yaml

          kubectl wait --for=condition=available --timeout=120s deployment/ingress-nginx-controller -n ingress-nginx

      - name: Deploy Ingress for NGINX Apps
        run: |
          kubectl apply -f ./Ingress/network.yaml

      - name: Deploy Istio Gateway for Doctobobo
        run: kubectl apply -f helm/istio/doctobobo-gateway.yaml

      - name: Deploy Istio VirtualService for Doctobobo
        run: kubectl apply -f helm/istio/doctobobo-virtualservice.yaml



      - name: Get Load Balancer IPs (NGINX & Istio)
        id: get-lb-ips
        run: |
          nginx_lb_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
      
          # Si l'IP est un nom DNS, extraire l'adresse IP réelle
          if [[ "$nginx_lb_ip" =~ [a-zA-Z] ]]; then
            nginx_lb_ip=$(dig +short "$nginx_lb_ip" | tail -n1)
          fi
      
          istio_lb_ip=$(kubectl get svc istio-ingress -n istio-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
      
          if [[ -z "$nginx_lb_ip" || "$nginx_lb_ip" == "null" ]]; then
            echo "Error: NGINX Load Balancer IP not found."
            exit 1
          fi
      
          if [[ -z "$istio_lb_ip" || "$istio_lb_ip" == "null" ]]; then
            echo "Error: Istio Load Balancer IP not found."
            exit 1
          fi
      
          echo "NGINX LB IP: $nginx_lb_ip"
          echo "Istio LB IP: $istio_lb_ip"
      
          echo "nginx_lb_ip=$nginx_lb_ip" >> $GITHUB_ENV
          echo "istio_lb_ip=$istio_lb_ip" >> $GITHUB_ENV


      - name: Install OVH Python SDK
        run: pip install ovh

      - name: Update OVH DNS Records
        env:
          OVH_APPLICATION_KEY: ${{ secrets.OVH_APP_KEY }}
          OVH_APPLICATION_SECRET: ${{ secrets.OVH_APP_SECRET }}
          OVH_CONSUMER_KEY: ${{ secrets.OVH_CONSUMER_KEY }}
          NGINX_LB_IP: ${{ env.nginx_lb_ip }}
          ISTIO_LB_IP: ${{ env.istio_lb_ip }}
        run: |
          python <<EOF
          import ovh
          import os

          client = ovh.Client(
              endpoint='ovh-eu',
              application_key=os.getenv('OVH_APPLICATION_KEY'),
              application_secret=os.getenv('OVH_APPLICATION_SECRET'),
              consumer_key=os.getenv('OVH_CONSUMER_KEY'),
          )

          domain = "my-soc.fr"
          nginx_lb_ip = os.getenv('NGINX_LB_IP')
          istio_lb_ip = os.getenv('ISTIO_LB_IP')

          apps_on_nginx = ["kibana2", "wazuh2"]
          app_on_istio = "doctobobo2"

          # Supprimer les anciens enregistrements
          for subdomain in apps_on_nginx + [app_on_istio]:
              records = client.get(f"/domain/zone/{domain}/record", fieldType="A", subDomain=subdomain)
              for record_id in records:
                  client.delete(f"/domain/zone/{domain}/record/{record_id}")
                  print(f"Deleted record {record_id} for {subdomain}")

          # Ajouter Doctobobo sur Istio
          client.post(f"/domain/zone/{domain}/record", fieldType="A", subDomain=app_on_istio, target=istio_lb_ip, ttl=0)
          print(f"Created Doctobobo record pointing to {istio_lb_ip}")

          # Ajouter les autres apps sur NGINX
          for subdomain in apps_on_nginx:
              client.post(f"/domain/zone/{domain}/record", fieldType="A", subDomain=subdomain, target=nginx_lb_ip, ttl=0)
              print(f"Created {subdomain} record pointing to {nginx_lb_ip}")

          client.post(f"/domain/zone/{domain}/refresh")
          print("DNS zone refreshed.")
          EOF


          

      # Étape 13 : Déploiement de cert-manager
      - name: Deploy cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager -n cert-manager --timeout=120s
          sleep 60

      # Étape 14 : Déploiement de ClusterIssuer
      - name: Deploy ClusterIssuer
        run: |
          kubectl apply -f ./Ingress/clusterIssuer.yaml -n default
