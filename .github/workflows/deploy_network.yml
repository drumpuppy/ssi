name: Deploy Network

on:
  workflow_dispatch:

jobs:
  deploy-network:
    runs-on: ubuntu-latest
    steps:
      # Étape 1 : Checkout du dépôt
      - name: Checkout repository
        uses: actions/checkout@v2

      # Étape 2 : Installer Helm (si nécessaire)
      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      # Étape 3 : Installer Scaleway CLI
      - name: Install Scaleway CLI
        run: |
          curl -s https://raw.githubusercontent.com/scaleway/scaleway-cli/master/scripts/get.sh | sh
          scw version

      # Étape 4 : Configurer Scaleway CLI
      - name: Configure Scaleway CLI
        env:
          SCW_ACCESS_KEY: ${{ secrets.SCW_ACCESS_KEY }}
          SCW_SECRET_KEY: ${{ secrets.SCW_SECRET_KEY }}
          SCW_DEFAULT_PROJECT_ID: ${{ secrets.SCW_PROJECT_ID }}
          SCW_DEFAULT_ORGANIZATION_ID: ${{ secrets.SCW_ORGANIZATION_ID }}
          SCW_DEFAULT_REGION: fr-par
          SCW_DEFAULT_ZONE: fr-par-1
        run: |
          mkdir -p ~/.config/scw
          touch ~/.config/scw/config.yaml
          scw config set access-key="${SCW_ACCESS_KEY}"
          scw config set secret-key="${SCW_SECRET_KEY}"
          scw config set default-organization-id="${SCW_DEFAULT_ORGANIZATION_ID}"
          scw config set default-project-id="${SCW_DEFAULT_PROJECT_ID}"
          scw config set default-region="${SCW_DEFAULT_REGION}"
          scw config set default-zone="${SCW_DEFAULT_ZONE}"

          # Vérification de la configuration
          scw config get default-region
          scw config get default-zone
          scw config get access-key

          # Exporter explicitement la région et la zone pour être certain qu'elles sont prises en compte
          echo "SCW_DEFAULT_REGION=${SCW_DEFAULT_REGION}" >> $GITHUB_ENV
          echo "SCW_DEFAULT_ZONE=${SCW_DEFAULT_ZONE}" >> $GITHUB_ENV

      - name: Create Flow Logs Bucket
        run: |
          BUCKET_NAME="flow-logs-$(date +%s)"
          echo "Creating bucket: $BUCKET_NAME"
          
          # Vérifier si la région est bien définie
          if [ -z "${SCW_DEFAULT_REGION}" ]; then
            echo "Error: SCW_DEFAULT_REGION is not set or is empty"
            exit 1
          fi
          
          # Créer un bucket Object Storage avec la bonne région
          scw object bucket create "$BUCKET_NAME" region="$SCW_DEFAULT_REGION" enable-versioning=false acl=private
          
          # Stocker le nom du bucket dans une variable d'environnement pour les étapes suivantes
          echo "BUCKET_NAME=$BUCKET_NAME" >> $GITHUB_ENV



      # Étape 5 : Configurer kubeconfig
      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          cluster_id=$(scw k8s cluster list name=k8s-cluster project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[0].id')
          if [[ -z "$cluster_id" || "$cluster_id" == "null" ]]; then
            echo "Error: Kubernetes cluster ID not found or is not valid."
            exit 1
          fi
          echo "Cluster ID: $cluster_id"
          scw k8s kubeconfig get "$cluster_id" > ~/.kube/config

      # Étape 7 : Clean des Ressources
      - name: Delete network resources
        continue-on-error: true
        run: |
          kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml -n ingress-nginx
          kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml -n cert-manager
          kubectl delete -f ./Ingress/clusterIssuer.yaml -n default
          kubectl delete -f ./Ingress/network.yaml
          kubectl delete ns ingress-nginx --force --grace-period=0

      # Étape 8 : Déploiement de NGINX Ingress Controller
      - name: Deploy NGINX Ingress Controller with Helm
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            -f ./helm/values-nginx.yaml

          kubectl wait --for=condition=available --timeout=120s deployment/ingress-nginx-controller -n ingress-nginx

      # Étape 9 : Déploiement de l'Ingress
      - name: Deploy Ingress
        run: |
          kubectl apply -f ./Ingress/network.yaml

      # Étape 10 : Récupérer l'IP du Load Balancer
      - name: Get Load Balancer IP
        id: get-lb-ip
        run: |
          lb_ip=$(scw lb ip list --output json | jq -r '.[0].ip_address')
          if [[ -z "$lb_ip" || "$lb_ip" == "null" ]]; then
            echo "Error: Load Balancer IP not found."
            exit 1
          fi
          echo "Load Balancer IP: $lb_ip"
          echo "lb_ip=$lb_ip" >> $GITHUB_ENV

      # Étape 11 : Installer ovh
      - name: Install OVH Python SDK
        run: |
          pip install ovh

      # Étape 12 : Gérer les domaines dans OVH
      - name: Update OVH DNS Records
        env:
          OVH_APPLICATION_KEY: ${{ secrets.OVH_APP_KEY }}
          OVH_APPLICATION_SECRET: ${{ secrets.OVH_APP_SECRET }}
          OVH_CONSUMER_KEY: ${{ secrets.OVH_CONSUMER_KEY }}
          LB_IP: ${{ env.lb_ip }}
        run: |
          python <<EOF
          import ovh
          import os
          
          # Configurer le client OVH
          client = ovh.Client(
              endpoint='ovh-eu',
              application_key=os.getenv('OVH_APPLICATION_KEY'),
              application_secret=os.getenv('OVH_APPLICATION_SECRET'),
              consumer_key=os.getenv('OVH_CONSUMER_KEY'),
          )
          
          # Domaine et IP du Load Balancer
          domain = "my-soc.fr"
          lb_ip = os.getenv('LB_IP')
          subdomains = ["prometheus2", "iris2", "kibana2", "doctobobo2"]
          
          # Récupérer et supprimer les enregistrements DNS existants
          for subdomain in subdomains:
              records = client.get(f"/domain/zone/{domain}/record", fieldType="A", subDomain=subdomain)
              for record_id in records:
                  client.delete(f"/domain/zone/{domain}/record/{record_id}")
                  print(f"Deleted record {record_id} for subdomain {subdomain}")
          
          # Ajouter de nouveaux enregistrements DNS
          for subdomain in subdomains:
              result = client.post(f"/domain/zone/{domain}/record", fieldType="A", subDomain=subdomain, target=lb_ip, ttl=0)
              print(f"Created new record: {result}")
          
          # Rafraîchir la zone DNS
          client.post(f"/domain/zone/{domain}/refresh")
          print("DNS zone refreshed.")
          EOF
          sleep 20

      # Étape 13 : Déploiement de cert-manager
      - name: Deploy cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager -n cert-manager --timeout=120s
          sleep 60

      # Étape 14 : Déploiement de ClusterIssuer
      - name: Deploy ClusterIssuer
        run: |
          kubectl apply -f ./Ingress/clusterIssuer.yaml -n default


      # Étape 6 : Activer les logs de flux réseau sur Scaleway
      - name: Enable Scaleway Flow Logs
        run: |
          BUCKET_NAME="flow-logs-$(date +%s)"
          scw object create-bucket "$BUCKET_NAME"
          
          GATEWAY_ID=$(scw vpc gateway list --output json | jq -r '.[0].id')
          if [[ -z "$GATEWAY_ID" || "$GATEWAY_ID" == "null" ]]; then
            echo "Error: No VPC Gateway found."
            exit 1
          fi
          echo "Found VPC Gateway ID: $GATEWAY_ID"
          
          scw vpc gateway update "$GATEWAY_ID" enable-flow-logs=true flow-logs-bucket="$BUCKET_NAME"
