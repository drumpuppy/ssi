name: Deploy Network

on:
  workflow_dispatch:

jobs:
  deploy-network:
    runs-on: ubuntu-latest
    steps:
      # Étape 1 : Checkout du dépôt
      - name: Checkout repository
        uses: actions/checkout@v2

      # Étape 2 : Installer Helm (si nécessaire)
      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      # Étape 3 : Installer Scaleway CLI
      - name: Install Scaleway CLI
        run: |
          curl -s https://raw.githubusercontent.com/scaleway/scaleway-cli/master/scripts/get.sh | sh
          scw version

      # Étape 4 : Configurer Scaleway CLI
      - name: Configure Scaleway CLI
        env:
          SCW_ACCESS_KEY: ${{ secrets.SCW_ACCESS_KEY }}
          SCW_SECRET_KEY: ${{ secrets.SCW_SECRET_KEY }}
          SCW_DEFAULT_PROJECT_ID: ${{ secrets.SCW_PROJECT_ID }}
          SCW_DEFAULT_ORGANIZATION_ID: ${{ secrets.SCW_ORGANIZATION_ID }}
          SCW_DEFAULT_REGION: fr-par
          SCW_DEFAULT_ZONE: fr-par-1
        run: |
          mkdir -p ~/.config/scw
          touch ~/.config/scw/config.yaml
          scw config set access-key="${SCW_ACCESS_KEY}"
          scw config set secret-key="${SCW_SECRET_KEY}"
          scw config set default-organization-id="${SCW_DEFAULT_ORGANIZATION_ID}"
          scw config set default-project-id="${SCW_DEFAULT_PROJECT_ID}"
          scw config set default-region="${SCW_DEFAULT_REGION}"
          scw config set default-zone="${SCW_DEFAULT_ZONE}"

          # Vérification de la configuration
          scw config get default-region
          scw config get default-zone
          scw config get access-key

          # Exporter explicitement la région et la zone pour être certain qu'elles sont prises en compte
          echo "SCW_DEFAULT_REGION=${SCW_DEFAULT_REGION}" >> $GITHUB_ENV
          echo "SCW_DEFAULT_ZONE=${SCW_DEFAULT_ZONE}" >> $GITHUB_ENV

      - name: Create Flow Logs Bucket
        run: |
          BUCKET_NAME="flow-logs-$(date +%s)"
          echo "Creating bucket: $BUCKET_NAME"
          
          # Vérifier si la région est bien définie
          if [ -z "${SCW_DEFAULT_REGION}" ]; then
            echo "Error: SCW_DEFAULT_REGION is not set or is empty"
            exit 1
          fi
          
          # Créer un bucket Object Storage avec la bonne région
          scw object bucket create "$BUCKET_NAME" region="$SCW_DEFAULT_REGION" enable-versioning=false acl=private
          
          # Stocker le nom du bucket dans une variable d'environnement pour les étapes suivantes
          echo "BUCKET_NAME=$BUCKET_NAME" >> $GITHUB_ENV



      # Étape 5 : Configurer kubeconfig
      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          cluster_id=$(scw k8s cluster list name=k8s-cluster project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[0].id')
          if [[ -z "$cluster_id" || "$cluster_id" == "null" ]]; then
            echo "Error: Kubernetes cluster ID not found or is not valid."
            exit 1
          fi
          echo "Cluster ID: $cluster_id"
          scw k8s kubeconfig get "$cluster_id" > ~/.kube/config

    
      # Étape 7 : Clean des Ressources
      - name: Delete network resources
        continue-on-error: true
        run: |
          kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml -n ingress-nginx
          kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml -n cert-manager
          kubectl delete -f ./Ingress/clusterIssuer.yaml -n default
          kubectl delete -f ./Ingress/network.yaml
          kubectl delete ns ingress-nginx --force --grace-period=0

      - name: Deploy Istio with Helm
        run: |
          helm repo add istio https://istio-release.storage.googleapis.com/charts
          helm repo update
          helm upgrade --install istio-base istio/base -n istio-system --create-namespace
          helm upgrade --install istiod istio/istiod -n istio-system
          helm upgrade --install istio-ingressgateway istio/gateway -n istio-system
          
          kubectl wait --for=condition=available --timeout=120s deployment/istiod -n istio-system


      # Étape 8 : Déploiement de NGINX Ingress Controller
      - name: Deploy NGINX Ingress Controller with Helm
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            -f ./helm/values-nginx.yaml

          kubectl wait --for=condition=available --timeout=120s deployment/ingress-nginx-controller -n ingress-nginx

      # Étape 9 : Déploiement de l'Ingress
      - name: Deploy Ingress
        run: |
          kubectl apply -f ./Ingress/network.yaml

      # Étape 10.5 : Déploiement de Wazuh avec Kustomize
      - name: Deploy Wazuh with Kustomize
        run: |
          kubectl apply -k ./helm/wazuh-kubernetes/wazuh/

      # Étape 10.6 : Vérification du déploiement de Wazuh
      - name: Check Wazuh Deployment
        run: |
          echo "Waiting for Wazuh pods to be ready..."
          kubectl wait --for=condition=ready pod -l app=wazuh-dashboard -n wazuh --timeout=300s
          kubectl get pods -n wazuh

      # Étape 10.7 : Appliquer Istio Gateway pour Doctobobo
      - name: Apply Istio Gateway for Doctobobo
        run: kubectl apply -f helm/istio/doctobobo-gateway.yaml
      
      # Étape 10.8 : Appliquer Istio VirtualService pour Doctobobo
      - name: Apply Istio VirtualService for Doctobobo
        run: kubectl apply -f helm/istio/doctobobo-virtualservice.yaml

      # Étape 10 : Récupérer l'IP du Load Balancer
      - name: Get Istio & NGINX Load Balancer IPs
        id: get-lb-ips
        run: |
          echo "Waiting for Load Balancer IPs to be assigned..."
      
          # Fonction pour récupérer l'IP du Load Balancer
          get_lb_ip() {
            local svc_name=$1
            local namespace=$2
            local ip=""
            local retries=10  # Nombre de tentatives (ajuster si nécessaire)
      
            for i in $(seq 1 $retries); do
              ip=$(kubectl get svc $svc_name -n $namespace -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
              if [[ -n "$ip" && "$ip" != "null" ]]; then
                echo "$svc_name IP: $ip"
                echo "$svc_name=$ip" >> $GITHUB_ENV
                return 0
              fi
              echo "Attempt $i/$retries: Waiting for $svc_name to get an IP..."
              sleep 30  # Attente entre chaque tentative
            done
      
            echo "Error: $svc_name Load Balancer IP not found."
            exit 1
          }
      
          # Récupérer l'IP d'Istio
          get_lb_ip istio-ingressgateway istio-system
      
          # Récupérer l'IP NGINX
          get_lb_ip ingress-nginx-controller ingress-nginx

          echo "Istio Load Balancer IP: $istio_lb_ip"
          echo "NGINX Load Balancer IP: $nginx_lb_ip"
          echo "istio_lb_ip=$istio_lb_ip" >> $GITHUB_ENV
          echo "nginx_lb_ip=$nginx_lb_ip" >> $GITHUB_ENV


      # Étape 11 : Installer ovh
      - name: Install OVH Python SDK
        run: |
          pip install ovh

      # Étape 12 : Gérer les domaines dans OVH
      - name: Update OVH DNS Records
        env:
          OVH_APPLICATION_KEY: ${{ secrets.OVH_APP_KEY }}
          OVH_APPLICATION_SECRET: ${{ secrets.OVH_APP_SECRET }}
          OVH_CONSUMER_KEY: ${{ secrets.OVH_CONSUMER_KEY }}
          ISTIO_LB_IP: ${{ env.istio_lb_ip }}
          NGINX_LB_IP: ${{ env.nginx_lb_ip }}
        run: |
          python <<EOF
          import ovh
          import os

          client = ovh.Client(
              endpoint='ovh-eu',
              application_key=os.getenv('OVH_APPLICATION_KEY'),
              application_secret=os.getenv('OVH_APPLICATION_SECRET'),
              consumer_key=os.getenv('OVH_CONSUMER_KEY'),
          )

          domain = "my-soc.fr"
          istio_ip = os.getenv('ISTIO_LB_IP')
          nginx_ip = os.getenv('NGINX_LB_IP')

          # Mapping des subdomains
          subdomains_mapping = {
              "doctobobo2": istio_ip,
              "kibana2": nginx_ip,
              "wazuh2": nginx_ip
          }

          # Suppression des anciens enregistrements
          for subdomain, ip in subdomains_mapping.items():
              records = client.get(f"/domain/zone/{domain}/record", fieldType="A", subDomain=subdomain)
              for record_id in records:
                  client.delete(f"/domain/zone/{domain}/record/{record_id}")
                  print(f"Deleted record {record_id} for subdomain {subdomain}")

          # Création des nouveaux enregistrements
          for subdomain, ip in subdomains_mapping.items():
              result = client.post(f"/domain/zone/{domain}/record", fieldType="A", subDomain=subdomain, target=ip, ttl=0)
              print(f"Created new record for {subdomain}: {result}")

          client.post(f"/domain/zone/{domain}/refresh")
          print("DNS zone refreshed.")
          EOF
          sleep 20


      # Étape 13 : Déploiement de cert-manager
      - name: Deploy cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager -n cert-manager --timeout=120s
          sleep 60

      # Étape 14 : Déploiement de ClusterIssuer
      - name: Deploy ClusterIssuer
        run: |
          kubectl apply -f ./Ingress/clusterIssuer.yaml -n default

