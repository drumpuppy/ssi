name: Deploy Helm Charts

on:
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      - name: Install Scaleway CLI
        run: |
          curl -s https://raw.githubusercontent.com/scaleway/scaleway-cli/master/scripts/get.sh | sh
          scw version

      - name: Configure Scaleway CLI
        env:
          SCW_ACCESS_KEY: ${{ secrets.SCW_ACCESS_KEY }}
          SCW_SECRET_KEY: ${{ secrets.SCW_SECRET_KEY }}
          SCW_DEFAULT_PROJECT_ID: ${{ secrets.SCW_PROJECT_ID }}
          SCW_DEFAULT_ORGANIZATION_ID: ${{ secrets.SCW_ORGANIZATION_ID }}
          SCW_DEFAULT_REGION: fr-par
          SCW_DEFAULT_ZONE: fr-par-1
        run: |
          mkdir -p ~/.config/scw
          touch ~/.config/scw/config.yaml
          scw config set access-key="${SCW_ACCESS_KEY}"
          scw config set secret-key="${SCW_SECRET_KEY}"
          scw config set default-organization-id="${SCW_DEFAULT_ORGANIZATION_ID}"
          scw config set default-project-id="${SCW_DEFAULT_PROJECT_ID}"
          scw config set default-region="${SCW_DEFAULT_REGION}"
          scw config set default-zone="${SCW_DEFAULT_ZONE}"

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          cluster_id=$(scw k8s cluster list name=k8s-cluster project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[0].id')
          if [[ -z "$cluster_id" || "$cluster_id" == "null" ]]; then
            echo "Error: Kubernetes cluster ID not found or is not valid."
            exit 1
          fi
          echo "Cluster ID: $cluster_id"
          scw k8s kubeconfig get "$cluster_id" > ~/.kube/config

      - name: Uninstall All Helm Charts
        continue-on-error: true
        run: |
          helm uninstall prometheus --namespace default
          helm uninstall prometheus-operator --namespace default
          helm uninstall logstash --namespace default
          helm uninstall filebeat --namespace default
          helm uninstall elasticsearch --namespace default
          helm uninstall istio-ingress --namespace istio-system
          helm uninstall istiod --namespace istio-system
          helm uninstall kibana --namespace default
      
      - name: Clean Up Kibana Resources
        continue-on-error: true
        run: |
          echo "Attempting to clean up Kibana resources..."
          kubectl delete sa post-delete-kibana-kibana -n default || echo "ServiceAccount already deleted or not found"
          kubectl delete cm kibana-kibana-helm-scripts -n default || echo "ConfigMap already deleted or not found"
          kubectl delete job post-delete-kibana-kibana -n default || echo "Job already deleted or not found"
          kubectl delete role post-delete-kibana-kibana -n default || echo "Role already deleted or not found"
          kubectl delete rolebinding post-delete-kibana-kibana -n default || echo "RoleBinding already deleted or not found"
          kubectl delete secret kibana-kibana-es-token -n default || echo "Secret already deleted or not found"
          kubectl delete secret sh.helm.release.v1.kibana.v1 -n default || echo "Helm release secret already deleted or not found"
      
      - name: Detach and Delete All Volumes
        run: |
          # Get all volumes in the project
          volume_ids=$(scw instance volume list project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[].id')
          if [ -z "$volume_ids" ]; then
            echo "No block storage volumes found."
          else
            for volume_id in $volume_ids; do
              # Check if the volume is attached to a server
              attached_server=$(scw instance volume get "$volume_id" --output json | jq -r '.server.id')
              if [ "$attached_server" != "null" ]; then
                echo "Detaching volume $volume_id from server $attached_server..."
                scw instance volume detach "$volume_id" || echo "Failed to detach volume $volume_id from server $attached_server."
              fi
              # Attempt to delete the volume
              echo "Deleting volume $volume_id..."
              scw instance volume delete "$volume_id" || echo "Failed to delete volume $volume_id. It may be in use."
            done
          fi

      - name: Delete All Block Storage Volumes
        run: |
          volume_ids=$(scw block volume list project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[].id')
          if [ -z "$volume_ids" ]; then
            echo "No block storage volumes found."
          else
            for volume_id in $volume_ids; do
              echo "Deleting volume $volume_id..."
              scw block volume delete volume-id=$volume_id zone=fr-par-1 || echo "Failed to delete volume $volume_id. It may be in use."
            done
          fi

      - name: Deploy Elasticsearch with Helm
        run: |
          helm repo add elastic https://helm.elastic.co
          helm repo update
          helm install elasticsearch elastic/elasticsearch \
            --create-namespace \
            --namespace default \
            --values ./helm/values-elasticsearch.yaml

      - name: Wait for Elasticsearch to be ready
        run: |
          echo "Waiting for Elasticsearch to be ready..."
          for i in {1..30}; do
            if kubectl get pods -n default -l app=elasticsearch-master -o jsonpath='{.items[0].status.containerStatuses[0].ready}' | grep -q 'true'; then
              echo "Elasticsearch is running."
              break
            fi
            echo "Elasticsearch is not ready. Retrying in 10 seconds..."
            sleep 10
          done
          # If Elasticsearch is not ready after 5 minutes, exit with an error
          if ! kubectl get pods -n default -l app=elasticsearch-master -o jsonpath='{.items[0].status.containerStatuses[0].ready}' | grep -q 'true'; then
            echo "Error: Elasticsearch did not start successfully within the timeout period."
            exit 1
          fi


      - name: Deploy Kibana with Helm
        run: |
          helm install kibana elastic/kibana \
            --namespace default \
            --values ./helm/values-kibana.yaml

      - name: Deploy Logstash with Helm
        run: |
          helm install logstash elastic/logstash \
            --namespace default \
            --values ./helm/values-logstash.yaml

      - name: Deploy Filebeat with Helm
        run: |
          helm install filebeat elastic/filebeat \
            --namespace default \
            --values ./helm/values-filebeat.yaml

      - name: Deploy Prometheus with Helm
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          helm install prometheus prometheus-community/prometheus \
            --namespace default \
            --values ./helm/values-prometheus.yaml

      - name: Deploy Prometheus Operator
        run: |
          helm install prometheus-operator prometheus-community/kube-prometheus-stack --namespace default


      - name: Deploy Falco with Helm
        run: |
          kubectl create namespace falco || true
          helm install falco falcosecurity/falco --namespace falco -f values-falco.yaml

      - name: Deploy Istio with Helm
        run: |
          helm repo add istio https://istio-release.storage.googleapis.com/charts
          helm repo update
          kubectl create namespace istio-system || true
          helm install istiod istio/istiod --namespace istio-system \
            --values ./helm/values-istio.yaml
          helm install istio-ingress istio/gateway --namespace istio-system \
            --values ./helm/values-istio-ingress.yaml



