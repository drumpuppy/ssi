name: Restart Pod on Suspicious Activity
on:
  workflow_dispatch:
    inputs:
      pod:
        description: "Pod to restart"
        required: true
      namespace:
        description: "Kubernetes Namespace"
        required: true

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Install Scaleway CLI
        run: |
          curl -s https://raw.githubusercontent.com/scaleway/scaleway-cli/master/scripts/get.sh | sh
          scw version

      - name: Configure Scaleway CLI
        env:
          SCW_ACCESS_KEY: ${{ secrets.SCW_ACCESS_KEY }}
          SCW_SECRET_KEY: ${{ secrets.SCW_SECRET_KEY }}
          SCW_DEFAULT_PROJECT_ID: ${{ secrets.SCW_PROJECT_ID }}
          SCW_DEFAULT_ORGANIZATION_ID: ${{ secrets.SCW_ORGANIZATION_ID }}
          SCW_DEFAULT_REGION: fr-par
          SCW_DEFAULT_ZONE: fr-par-1
        run: |
          mkdir -p ~/.config/scw
          touch ~/.config/scw/config.yaml
          scw config set access-key="${SCW_ACCESS_KEY}"
          scw config set secret-key="${SCW_SECRET_KEY}"
          scw config set default-organization-id="${SCW_DEFAULT_ORGANIZATION_ID}"
          scw config set default-project-id="${SCW_DEFAULT_PROJECT_ID}"
          scw config set default-region="${SCW_DEFAULT_REGION}"
          scw config set default-zone="${SCW_DEFAULT_ZONE}"

          export SCW_DEFAULT_ZONE=$(scw config get default-zone)
          echo "SCW_DEFAULT_ZONE=$SCW_DEFAULT_ZONE" >> $GITHUB_ENV
          export SCW_SECRET_KEY="${SCW_SECRET_KEY}"
          echo "SCW_SECRET_KEY=${SCW_SECRET_KEY}" >> $GITHUB_ENV
          export SCW_DEFAULT_PROJECT_ID=$(scw config get default-project-id)
          echo "SCW_DEFAULT_PROJECT_ID=${SCW_DEFAULT_PROJECT_ID}" >> $GITHUB_ENV

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          cluster_id=$(scw k8s cluster list name=k8s-cluster project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[0].id')
          if [[ -z "$cluster_id" || "$cluster_id" == "null" ]]; then
            echo "Error: Kubernetes cluster ID not found or is not valid."
            exit 1
          fi
          echo "Cluster ID: $cluster_id"
          scw k8s kubeconfig get "$cluster_id" > ~/.kube/config

      - name: Verify kubectl and context
        run: |
          kubectl version --client
          kubectl config current-context
          kubectl get nodes

      - name: Restart suspicious pod
        run: |
          echo "Restarting pod: ${{ github.event.inputs.pod }} in namespace: ${{ github.event.inputs.namespace }}"
          kubectl delete pod "${{ github.event.inputs.pod }}" -n "${{ github.event.inputs.namespace }}"
          echo "Pod ${{ github.event.inputs.pod }} has been deleted. Kubernetes will recreate it automatically."
