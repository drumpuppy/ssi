name: /!\ Destroy K8s Cluster and Helm Charts /!\

on:
  workflow_dispatch:

jobs:
  destroy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      - name: Install Scaleway CLI
        run: |
          curl -s https://raw.githubusercontent.com/scaleway/scaleway-cli/master/scripts/get.sh | sh
          scw version

      - name: Configure Scaleway CLI
        env:
          SCW_ACCESS_KEY: ${{ secrets.SCW_ACCESS_KEY }}
          SCW_SECRET_KEY: ${{ secrets.SCW_SECRET_KEY }}
          SCW_DEFAULT_PROJECT_ID: ${{ secrets.SCW_PROJECT_ID }}
          SCW_DEFAULT_ORGANIZATION_ID: ${{ secrets.SCW_ORGANIZATION_ID }}
          SCW_DEFAULT_REGION: fr-par
          SCW_DEFAULT_ZONE: fr-par-1
        run: |
          mkdir -p ~/.config/scw
          touch ~/.config/scw/config.yaml
          scw config set access-key="${SCW_ACCESS_KEY}"
          scw config set secret-key="${SCW_SECRET_KEY}"
          scw config set default-organization-id="${SCW_DEFAULT_ORGANIZATION_ID}"
          scw config set default-project-id="${SCW_DEFAULT_PROJECT_ID}"
          scw config set default-region="${SCW_DEFAULT_REGION}"
          scw config set default-zone="${SCW_DEFAULT_ZONE}"


      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          cluster_id=$(scw k8s cluster list name=k8s-cluster project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[0].id')
          if [[ -z "$cluster_id" || "$cluster_id" == "null" ]]; then
            echo "Error: Kubernetes cluster ID not found or is not valid."
            exit 1
          fi
          scw k8s kubeconfig get "$cluster_id" > ~/.kube/config

      - name: Detach and Delete All Volumes
        run: |
          # Get all volumes in the project
          volume_ids=$(scw instance volume list project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[].id')
          if [ -z "$volume_ids" ]; then
            echo "No block storage volumes found."
          else
            for volume_id in $volume_ids; do
              # Check if the volume is attached to a server
              attached_server=$(scw instance volume get "$volume_id" --output json | jq -r '.server.id')
              if [ "$attached_server" != "null" ]; then
                echo "Detaching volume $volume_id from server $attached_server..."
                scw instance volume detach "$volume_id" || echo "Failed to detach volume $volume_id from server $attached_server."
              fi
              # Attempt to delete the volume
              echo "Deleting volume $volume_id..."
              scw instance volume delete "$volume_id" || echo "Failed to delete volume $volume_id. It may be in use."
            done
          fi
    
      - name: Uninstall All Helm Charts
        continue-on-error: true
        run: |
          namespaces=("default" "istio-system")
          for ns in "${namespaces[@]}"; do
            if helm list -n $ns | grep -q $ns; then
              echo "Uninstalling charts in namespace $ns..."
              helm uninstall $(helm list -n $ns -q) --namespace $ns || true
            fi
          done
      
      - name: Clean Up Kibana Resources
        continue-on-error: true
        run: |
          echo "Attempting to clean up Kibana resources..."
          kubectl delete sa post-delete-kibana-kibana -n default || echo "ServiceAccount already deleted or not found"
          kubectl delete cm kibana-kibana-helm-scripts -n default || echo "ConfigMap already deleted or not found"
          kubectl delete job post-delete-kibana-kibana -n default || echo "Job already deleted or not found"
          kubectl delete role post-delete-kibana-kibana -n default || echo "Role already deleted or not found"
          kubectl delete rolebinding post-delete-kibana-kibana -n default || echo "RoleBinding already deleted or not found"
          kubectl delete secret kibana-kibana-es-token -n default || echo "Secret already deleted or not found"
          kubectl delete secret sh.helm.release.v1.kibana.v1 -n default || echo "Helm release secret already deleted or not found"



      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v1
        with:
          cli_config_credentials_hostname: app.terraform.io
          cli_config_credentials_token: ${{ secrets.TF_CLOUD_TOKEN }}
          terraform_version: latest

      - name: Terraform Init for Destroy
        working-directory: ./infra
        env:
          TF_VAR_scw_access_key: ${{ secrets.SCW_ACCESS_KEY }}
          TF_VAR_scw_secret_key: ${{ secrets.SCW_SECRET_KEY }}
          TF_VAR_scw_project_id: ${{ secrets.SCW_PROJECT_ID }}
        run: terraform init

      - name: Terraform Destroy
        working-directory: ./infra
        env:
          TF_VAR_scw_access_key: ${{ secrets.SCW_ACCESS_KEY }}
          TF_VAR_scw_secret_key: ${{ secrets.SCW_SECRET_KEY }}
          TF_VAR_scw_project_id: ${{ secrets.SCW_PROJECT_ID }}
          TF_LOG: DEBUG
        run: terraform destroy -auto-approve

      - name: Delete All Block Storage Volumes
        run: |
          volume_ids=$(scw block volume list project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[].id')
          if [ -z "$volume_ids" ]; then
            echo "No block storage volumes found."
          else
            for volume_id in $volume_ids; do
              echo "Deleting volume $volume_id..."
              scw block volume delete volume-id=$volume_id zone=fr-par-1 || echo "Failed to delete volume $volume_id. It may be in use."
            done
          fi

      


