name: Restore PVC

on:
  workflow_dispatch:

jobs:
  restore_pvc:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      - name: Install Scaleway CLI
        run: |
          curl -s https://raw.githubusercontent.com/scaleway/scaleway-cli/master/scripts/get.sh | sh
          scw version

      - name: Configure Scaleway CLI
        env:
          SCW_ACCESS_KEY: ${{ secrets.SCW_ACCESS_KEY }}
          SCW_SECRET_KEY: ${{ secrets.SCW_SECRET_KEY }}
          SCW_DEFAULT_PROJECT_ID: ${{ secrets.SCW_PROJECT_ID }}
          SCW_DEFAULT_ORGANIZATION_ID: ${{ secrets.SCW_ORGANIZATION_ID }}
          SCW_DEFAULT_REGION: fr-par
          SCW_DEFAULT_ZONE: fr-par-1
        run: |
          mkdir -p ~/.config/scw
          touch ~/.config/scw/config.yaml
          scw config set access-key="${SCW_ACCESS_KEY}"
          scw config set secret-key="${SCW_SECRET_KEY}"
          scw config set default-organization-id="${SCW_DEFAULT_ORGANIZATION_ID}"
          scw config set default-project-id="${SCW_DEFAULT_PROJECT_ID}"
          scw config set default-region="${SCW_DEFAULT_REGION}"
          scw config set default-zone="${SCW_DEFAULT_ZONE}"

          export SCW_DEFAULT_ZONE=$(scw config get default-zone)
          echo "SCW_DEFAULT_ZONE=$SCW_DEFAULT_ZONE" >> $GITHUB_ENV
          export SCW_SECRET_KEY="${SCW_SECRET_KEY}"
          echo "SCW_SECRET_KEY=${SCW_SECRET_KEY}" >> $GITHUB_ENV
          export SCW_DEFAULT_PROJECT_ID=$(scw config get default-project-id)
          echo "SCW_DEFAULT_PROJECT_ID=${SCW_DEFAULT_PROJECT_ID}" >> $GITHUB_ENV

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          cluster_id=$(scw k8s cluster list name=k8s-cluster project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[0].id')
          if [[ -z "$cluster_id" || "$cluster_id" == "null" ]]; then
            echo "Error: Kubernetes cluster ID not found or is not valid."
            exit 1
          fi
          echo "Cluster ID: $cluster_id"
          scw k8s kubeconfig get "$cluster_id" > ~/.kube/config

      - name: Clean Up
        continue-on-error: true
        run: |
          if helm list -n default | grep -q 'elasticsearch'; then
            helm uninstall elasticsearch --namespace default
          else
            echo "Elastic not installed in the default namespace."
          fi

          if helm list -n default | grep -q 'logstash'; then
            helm uninstall logstash --namespace default
          else
            echo "Logstash not installed in the default namespace."
          fi

          if helm list -n default | grep -q 'filebeat'; then
            helm uninstall filebeat --namespace default
          else
            echo "Filebeat not installed in the default namespace."
          fi

          if helm list -n default | grep -q 'kibana'; then
            echo "Uninstalling Kibana in the background..."
            helm uninstall kibana --namespace default --wait=false &
          else
            echo "Kibana not installed in the default namespace."
          fi

          kubectl delete -f ./doctobobo/backend/mysql-deployment.yaml -n doctobobo

      - name: Clean Up Kibana Resources
        continue-on-error: true
        run: |
          echo "Attempting to clean up Kibana resources..."
          kubectl delete sa post-delete-kibana-kibana -n default || echo "ServiceAccount already deleted or not found"
          kubectl delete cm kibana-kibana-helm-scripts -n default || echo "ConfigMap already deleted or not found"
          kubectl delete job post-delete-kibana-kibana -n default || echo "Job already deleted or not found"
          kubectl delete role post-delete-kibana-kibana -n default || echo "Role already deleted or not found"
          kubectl delete rolebinding post-delete-kibana-kibana -n default || echo "RoleBinding already deleted or not found"
          kubectl delete secret kibana-kibana-es-token -n default || echo "Secret already deleted or not found"
          kubectl delete secret sh.helm.release.v1.kibana.v1 -n default || echo "Helm release secret already deleted or not found"

      - name: Fetch Scaleway Volumes
        run: |
          echo "Fetching Scaleway volumes..."
          curl -s -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
               "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/volumes" | tee volumes.json
          
          if [ ! -s volumes.json ]; then
            echo "‚ùå Error: volumes.json is empty or missing. Exiting..."
            exit 1
          fi

      - name: Restore PVCs from snapshots
        run: |
          PVC_LIST=("default elasticsearch-master-elasticsearch-master-0" "doctobobo mysql-pvc")

          for entry in "${PVC_LIST[@]}"; do
            namespace=$(echo $entry | awk '{print $1}')
            pvc=$(echo $entry | awk '{print $2}')

            # Get the latest snapshot ID for the PVC
            latest_snapshot=$(curl -s -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
              "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/snapshots" | \
              jq -r --arg pvc "$pvc" '.snapshots[] | select(.name | contains($pvc)) | select(.status=="available") | .id' | sort | tail -n1)

            if [ -z "$latest_snapshot" ] || [ "$latest_snapshot" == "null" ]; then
              echo "‚ö†Ô∏è No available snapshot found for PVC $pvc, skipping..."
              continue
            fi

            echo "‚úÖ Found snapshot for $pvc: $latest_snapshot. Restoring..."

            # **Step 1: Ensure Deletion of Existing K8s PVC**
            if kubectl get pvc $pvc -n $namespace &>/dev/null; then
              pvc_status=$(kubectl get pvc $pvc -n $namespace -o jsonpath='{.status.phase}')
              if [ "$pvc_status" == "Terminating" ]; then
                echo "‚ö†Ô∏è PVC $pvc is stuck in Terminating state! Fixing..."

                # Remove Finalizer
                finalizers=$(kubectl get pvc $pvc -n $namespace -o jsonpath='{.metadata.finalizers}')
                if [[ "$finalizers" == *"pvc-protection"* ]]; then
                  echo "üîß Removing finalizer from PVC $pvc..."
                  kubectl patch pvc $pvc -n $namespace --type=json -p '[{"op": "remove", "path": "/metadata/finalizers"}]'
                fi

                # Force delete the PVC
                echo "üóëÔ∏è Force deleting stuck PVC $pvc..."
                kubectl delete pvc $pvc -n $namespace --grace-period=0 --force
              else
                echo "üóëÔ∏è Deleting auto-created PVC $pvc in namespace $namespace..."
                kubectl delete pvc $pvc -n $namespace --ignore-not-found
              fi
            fi

            # **Step 2: Ensure Deletion of Existing K8s PV**
            if kubectl get pv restored-$pvc &>/dev/null; then
              echo "üóëÔ∏è Deleting existing PersistentVolume restored-$pvc..."
              kubectl delete pv restored-$pvc --ignore-not-found
            fi

            # **Step 3: Delete old Scaleway Block Storage Volume (if needed)**
            existing_volume_id=$(curl -s -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
              "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/volumes" | \
              jq -r --arg pvc "$pvc" '.volumes[] | select(.name | contains($pvc)) | .id')

            if [ -n "$existing_volume_id" ] && [ "$existing_volume_id" != "null" ]; then
              echo "üóëÔ∏è Deleting existing Scaleway volume for PVC $pvc..."
              curl -s -X DELETE -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
                "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/volumes/$existing_volume_id"
            fi

            # **Step 4: Restore the Volume from Snapshot in Scaleway**
            restored_volume=$(curl -s -X POST -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
              -H "Content-Type: application/json" \
              -d '{
                "name": "restored-'$pvc'",
                "from_snapshot": { "snapshot_id": "'$latest_snapshot'" },
                "project_id": "'$SCW_DEFAULT_PROJECT_ID'",
                "zone": "'$SCW_DEFAULT_ZONE'"
              }' \
              "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/volumes")

            restored_volume_id=$(echo "$restored_volume" | jq -r '.id')

            if [ -z "$restored_volume_id" ] || [ "$restored_volume_id" == "null" ]; then
              echo "‚ùå Failed to restore PVC $pvc from snapshot $latest_snapshot. Response:"
              echo "$restored_volume"
              continue
            fi

            echo "‚úÖ PVC $pvc restored with volume ID $restored_volume_id."

            # **Step 5: Wait for the Restored Volume to be Available**
            echo "‚è≥ Waiting for restored volume to become available..."
            for i in {1..10}; do
              volume_status=$(curl -s -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
                "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/volumes/$restored_volume_id" | \
                jq -r '.status')

              if [ "$volume_status" == "available" ]; then
                echo "‚úÖ Restored volume is now available!"
                break
              fi
              echo "üîÑ Waiting for volume to be available... ($i/10)"
              sleep 10
            done


      - name: Deploy Elasticsearch
        run: |
          helm repo add elastic https://helm.elastic.co
          helm repo update
          helm install elasticsearch elastic/elasticsearch \
            --create-namespace \
            --namespace default \
            --values ./helm/values-elasticsearch.yaml

      - name: Wait for Elasticsearch to be ready
        run: |
          echo "Waiting for Elasticsearch to be ready..."
          for i in {1..30}; do
            if kubectl get pods -n default -l app=elasticsearch-master -o jsonpath='{.items[0].status.containerStatuses[0].ready}' | grep -q 'true'; then
              echo "Elasticsearch is running."
              break
            fi
            echo "Elasticsearch is not ready. Retrying in 10 seconds..."
            sleep 10
          done
          # If Elasticsearch is not ready after 5 minutes, exit with an error
          if ! kubectl get pods -n default -l app=elasticsearch-master -o jsonpath='{.items[0].status.containerStatuses[0].ready}' | grep -q 'true'; then
            echo "Error: Elasticsearch did not start successfully within the timeout period."
            exit 1
          fi

      - name: Deploy Kibana
        run: |
          helm install kibana elastic/kibana \
            --namespace default \
            --values ./helm/values-kibana.yaml

      - name: Deploy Logstash
        run: |
          helm install logstash elastic/logstash \
            --namespace default \
            --values ./helm/values-logstash.yaml

      - name: Deploy Filebeat
        run: |
          helm install filebeat elastic/filebeat \
            --namespace default \
            --values ./helm/values-filebeat.yaml

      - name: redeploy other ressources
        continue-on-error: true
        run: |
          kubectl apply -f ./doctobobo/backend/mysql-deployment.yaml -n doctobobo
