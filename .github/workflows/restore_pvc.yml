name: Restore PVC

on:
  workflow_dispatch:

jobs:
  restore_pvc:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      - name: Install Scaleway CLI
        run: |
          curl -s https://raw.githubusercontent.com/scaleway/scaleway-cli/master/scripts/get.sh | sh
          scw version

      - name: Configure Scaleway CLI
        env:
          SCW_ACCESS_KEY: ${{ secrets.SCW_ACCESS_KEY }}
          SCW_SECRET_KEY: ${{ secrets.SCW_SECRET_KEY }}
          SCW_DEFAULT_PROJECT_ID: ${{ secrets.SCW_PROJECT_ID }}
          SCW_DEFAULT_ORGANIZATION_ID: ${{ secrets.SCW_ORGANIZATION_ID }}
          SCW_DEFAULT_REGION: fr-par
          SCW_DEFAULT_ZONE: fr-par-1
        run: |
          mkdir -p ~/.config/scw
          touch ~/.config/scw/config.yaml
          scw config set access-key="${SCW_ACCESS_KEY}"
          scw config set secret-key="${SCW_SECRET_KEY}"
          scw config set default-organization-id="${SCW_DEFAULT_ORGANIZATION_ID}"
          scw config set default-project-id="${SCW_DEFAULT_PROJECT_ID}"
          scw config set default-region="${SCW_DEFAULT_REGION}"
          scw config set default-zone="${SCW_DEFAULT_ZONE}"

          export SCW_DEFAULT_ZONE=$(scw config get default-zone)
          echo "SCW_DEFAULT_ZONE=$SCW_DEFAULT_ZONE" >> $GITHUB_ENV
          export SCW_SECRET_KEY="${SCW_SECRET_KEY}"
          echo "SCW_SECRET_KEY=${SCW_SECRET_KEY}" >> $GITHUB_ENV
          export SCW_DEFAULT_PROJECT_ID=$(scw config get default-project-id)
          echo "SCW_DEFAULT_PROJECT_ID=${SCW_DEFAULT_PROJECT_ID}" >> $GITHUB_ENV

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          cluster_id=$(scw k8s cluster list name=k8s-cluster project-id=${{ secrets.SCW_PROJECT_ID }} --output json | jq -r '.[0].id')
          if [[ -z "$cluster_id" || "$cluster_id" == "null" ]]; then
            echo "Error: Kubernetes cluster ID not found or is not valid."
            exit 1
          fi
          echo "Cluster ID: $cluster_id"
          scw k8s kubeconfig get "$cluster_id" > ~/.kube/config

      - name: Clean Up
        continue-on-error: true
        run: |
          if helm list -n default | grep -q 'elasticsearch'; then
            helm uninstall elasticsearch --namespace default
          else
            echo "Elastic not installed in the default namespace."
          fi
          kubectl delete -f ./doctobobo/backend/mysql-deployment.yaml -n doctobobo

      - name: Fetch Scaleway Volumes
        run: |
          echo "Fetching Scaleway volumes..."
          curl -s -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
               "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/volumes" | tee volumes.json
          
          if [ ! -s volumes.json ]; then
            echo "‚ùå Error: volumes.json is empty or missing. Exiting..."
            exit 1
          fi

      - name: Restore PVCs from snapshots
        run: |
          PVC_LIST=("default elasticsearch-master-elasticsearch-master-0" "doctobobo mysql-pvc")

          for entry in "${PVC_LIST[@]}"; do
            namespace=$(echo $entry | awk '{print $1}')
            pvc=$(echo $entry | awk '{print $2}')

            # Get the latest snapshot ID for the PVC
            latest_snapshot=$(curl -s -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
              "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/snapshots" | \
              jq -r --arg pvc "$pvc" '.snapshots[] | select(.name | contains($pvc)) | select(.status=="available") | .id' | sort | tail -n1)

            if [ -z "$latest_snapshot" ] || [ "$latest_snapshot" == "null" ]; then
              echo "‚ö†Ô∏è No available snapshot found for PVC $pvc, skipping..."
              continue
            fi

            echo "‚úÖ Found snapshot for $pvc: $latest_snapshot. Restoring..."

            # Detect stuck PVCs and remove finalizers if necessary
            if kubectl get pvc $pvc -n $namespace &>/dev/null; then
              pvc_status=$(kubectl get pvc $pvc -n $namespace -o jsonpath='{.status.phase}')
              if [ "$pvc_status" == "Terminating" ]; then
                echo "‚ö†Ô∏è PVC $pvc is stuck in Terminating state! Fixing..."

                # Remove Finalizer
                finalizers=$(kubectl get pvc $pvc -n $namespace -o jsonpath='{.metadata.finalizers}')
                if [[ "$finalizers" == *"pvc-protection"* ]]; then
                  echo "üîß Removing finalizer from PVC $pvc..."
                  kubectl patch pvc $pvc -n $namespace --type=json -p '[{"op": "remove", "path": "/metadata/finalizers"}]'
                fi

                # Force delete the PVC
                echo "üóëÔ∏è Force deleting stuck PVC $pvc..."
                kubectl delete pvc $pvc -n $namespace --grace-period=0 --force
              else
                echo "üóëÔ∏è Deleting auto-created PVC $pvc in namespace $namespace..."
                kubectl delete pvc $pvc -n $namespace --ignore-not-found
              fi
            fi

            # Restore the volume from snapshot in Scaleway
            restored_volume=$(curl -s -X POST -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
              -H "Content-Type: application/json" \
              -d '{
                "name": "restored-'$pvc'",
                "from_snapshot": "'$latest_snapshot'",
                "project_id": "'$SCW_DEFAULT_PROJECT_ID'",
                "zone": "'$SCW_DEFAULT_ZONE'"
              }' \
              "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/volumes")

            restored_volume_id=$(echo "$restored_volume" | jq -r '.id')

            if [ -z "$restored_volume_id" ] || [ "$restored_volume_id" == "null" ]; then
              echo "‚ùå Failed to restore PVC $pvc from snapshot $latest_snapshot. Response:"
              echo "$restored_volume"
              continue
            fi

            echo "‚úÖ PVC $pvc restored with volume ID $restored_volume_id."

            # Wait for the restored volume to become available
            echo "‚è≥ Waiting for restored volume to become available..."
            for i in {1..10}; do
              volume_status=$(curl -s -H "X-Auth-Token: ${SCW_SECRET_KEY}" \
                "https://api.scaleway.com/block/v1alpha1/zones/${SCW_DEFAULT_ZONE}/volumes/$restored_volume_id" | \
                jq -r '.status')

              if [ "$volume_status" == "available" ]; then
                echo "‚úÖ Restored volume is now available!"
                break
              fi
              echo "üîÑ Waiting for volume to be available... ($i/10)"
              sleep 10
            done

            # Manually re-create the PVC in Kubernetes, binding it to the restored volume
            echo "üîÑ Creating PersistentVolume for restored PVC..."
            kubectl create pv restored-$pvc \
              --namespace=$namespace \
              --storage=30Gi \
              --access-mode=ReadWriteOnce \
              --storage-class=scaleway-block-storage \
              --volume-handle=$restored_volume_id \
              --persistent-volume-reclaim-policy=Retain

            echo "üîÑ Creating PersistentVolumeClaim for restored PVC..."
            kubectl create pvc $pvc \
              --namespace=$namespace \
              --storage=30Gi \
              --access-mode=ReadWriteOnce \
              --storage-class=scaleway-block-storage \
              --volume-name=restored-$pvc
          done

      - name: redeploy ressources
        continue-on-error: true
        run: |
          kubectl apply -f ./doctobobo/backend/mysql-deployment.yaml -n doctobobo
          helm repo add elastic https://helm.elastic.co
          helm repo update
          helm install elasticsearch elastic/elasticsearch \
            --create-namespace \
            --namespace default \
            --values ./helm/values-elasticsearch.yaml
